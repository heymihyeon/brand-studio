---
title: 'Comprehensive Test Runner'
read_only: true
type: 'command'
version: '1.0.0'
---

# ğŸ§ª '/test-all unit' ë˜ëŠ” '/test-all' ê°™ì´ í”„ë¡œì íŠ¸ì˜ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰

Purpose: Execute all test suites with intelligent retry and coverage validation
Target: $ARGUMENTS (test scope: 'unit'|'integration'|'e2e'|'all'|specific path)

**IMPORTANT**: All user-facing messages, questions, and results must be displayed in Korean.

## Step 1: Determine Test Scope

### 1.0 TDD Workflow Check
```
Remind about TDD cycle:
1. Write Test â†’ 2. Implement â†’ 3. Refactor â†’ Repeat

Verify test files exist for:
- modules/orchestrator/__tests__/
- modules/executor/__tests__/
- modules/analyzer/__tests__/
```

## Step 1: Determine Test Scope

### 1.1 Parse Target
```
IF $ARGUMENTS == 'all' OR empty:
  Run all test types
ELSE:
  Run specified tests
```

### 1.2 Prepare Environment
```
Verify:
- SQLite test database ready
- Environment variables set (.env.local)
  - API keys (follow security rules from {project-root}/CLAUDE.md Section 4)
  - KUBECONFIG (for k8s tests)
- Clean test state
- Mock Kubernetes cluster available (for e2e)
```

## Step 2: Execute Tests

### 2.1 Run Test Suites
```
Execute in order:
1. Lint and TypeScript checks
   - npm run lint
   - npm run typecheck
2. Unit tests
   - npm run test:unit
3. Integration tests
   - npm run test:integration
4. E2E tests (if test cluster available)
   - npm run test:e2e
5. Build validation
   - npm run build
6. Docker validation
   - npm run docker:build
   - npm run docker:run

Handle failures:
- Stop on lint/type errors
- Continue on test failures
- Report all issues
```

### 2.2 Collect Results
```
Track:
- Pass/fail counts
- Execution times
- Coverage metrics
- Flaky tests
```

## Step 3: Coverage Analysis

### 3.1 Check Coverage
```
Compare against AIDA project thresholds:
- Overall: 80% minimum
- Core modules: 90% (orchestrator, analyzer)
- Safety-critical: 100% (command validation)
- New code: 95% minimum
- Current status: Track actual coverage
```

## Step 4: Generate Report

### 4.1 Test Summary
```
Display in Korean:
í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼:
- ì „ì²´: Xê°œ ì¤‘ Yê°œ í†µê³¼
- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: X/Y
- í†µí•© í…ŒìŠ¤íŠ¸: X/Y
- E2E í…ŒìŠ¤íŠ¸: X/Y
- ì»¤ë²„ë¦¬ì§€: XX% (ëª©í‘œ: 80%)
- ì½”ì–´ ëª¨ë“ˆ ì»¤ë²„ë¦¬ì§€: XX% (ëª©í‘œ: 90%)
- ì•ˆì „ í¬ë¦¬í‹°ì»¬ ì½”ë“œ: XX% (ëª©í‘œ: 100%)

ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:
[List failed tests with error summary]

ë‹¤ìŒ ë‹¨ê³„:
[Recommendations]
```

## Error Handling

### On Catastrophic Failure
```
If all tests fail:
1. Check environment
   - Verify .env.local exists
   - Check SQLite database
   - Validate Kubernetes access
2. Reset test data
   - npm run db:migrate -- --reset
3. Run diagnostic mode
   - Check LLM API connectivity
   - Verify kubectl commands
4. Report issue in Korean
```

### AIDA-Specific Test Categories
```
Core Test Scenarios:
1. Pod CrashLoopBackOff simulation
2. Network connectivity issue tests
3. High resource usage scenarios
4. LLM rate limiting tests
5. Fallback strategy validation
6. Command safety validation (100% required)
```